Bots are programmes that can automate a number of diverse tasks. Adversaries can misuse bots to control accounts on Online Social Networks (OSNs) and potentially sway peoples' opinions by spreading misinformation and propaganda. These bots can for example be used by politicians to inflate the level of support for a political candidate.
This bachelor thesis explains what bots are, how they can be misused in OSNs, what vulnerabilities OSNs exhibit and what can be done to detect bots. Some bots are created to be helpful to humans. Other bots can be misused by adversaries to create a seemingly realistic OSN account. The account can then be utilised to connect with other users and share information with them (through re-posting or creating synthetic messages). In order to prevent bot infiltrations, current weaknesses in OSNs would need to be fixed. Examples given for these are the use of ineffective CAPTCHAs, the use of Sybil accounts and fake profiles, crawlable social graphs, and exploitable platforms and APIs. Since OSN security defences are more advantageous in detecting bot infiltrations, rather than preventing them, this thesis focuses more on detecting bots. 
An adversary's goal typically is to use socialbots to influence a wide range of users and share content with them. In doing so, these bots exhibit certain behavioural patterns, which can be exploited to detect them. Examples for such patterns include excessive automatically created content, presence of malicious URLs, aggressive following behaviour, or long online sessions. This paper further reviews the results of the "DARPA Twitter Bot Detection Challenge” and the industry's goto platform “Botometer” (formerly "BotOrNot"), which use feature-based detection to classify Twitter accounts as bots or not bots. Other detection systems are examined, such as the "Associative Affinity Factor Analysis" (AAFA), which achieved 0.96 accuracy, while BotOrNot only achieved 0.66.
